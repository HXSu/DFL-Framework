{
  "per_device_train_batch_size": 8,
  "per_device_eval_batch_size": 8,

  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 2,

  "num_train_epochs": 10, 

  "learning_rate": 1e-3,

  "save_total_limit": 2,
  "logging_steps": 100,

  "output_dir": "trainer_checkpoint",

  "optim": "paged_adamw_8bit",
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,

  "save_strategy": "epoch",
  "evaluation_strategy": "steps"
}
